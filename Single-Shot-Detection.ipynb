{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing For SSD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the data\n",
    "2. [DOING .. ] Create the base network.\n",
    "   1. [DOING .. ] Create the base network. \n",
    "   2. [DOING .. ] Base network should send out to final predctions a class predction (1-hot encoded) and a location prediction (a,b,c,d) relative sizes. \n",
    "   2. [DOING .. ] Figure out which layers will output detections to prediction array. \n",
    "   2. [DOING .. ] Figure out the size of the feature maps in each case above. \n",
    "   3. [DOING .. ] Figure out number of outputs in final array \n",
    "3. Preprocess\n",
    "  1. Create a function to pull a image and its data \n",
    "  2. create a hash of Ground truth image -> bounding box, class\n",
    "  3. Create the default box sizes.\n",
    "  3. For each feature map square, create the different default boxes. \n",
    "     1. For each default box \n",
    "         2. See if it matches with the ground truth.\n",
    "             3. YES - then set class value to 1 (pedestrian)\n",
    "             4. NO - set class value to 0 (background)\n",
    "         3. Calculate how much negative boxes are created. If too many ( greater than 2/3 ratio) then drop from training set. \n",
    "         4. Create a list of [y_box_coords, y_confidence (1-hot encoded class),]\n",
    "4. Create one prediction through the network. \n",
    "4. Data Augmenation \n",
    "   1. Use the entire orignal input image\n",
    "   2. Sample a patch so that the mininmum jaccard overlap with the object is 0.1,0.3,0.5,0.7,0.9\n",
    "   3. Randomly Sample a patch. \n",
    "   4. This new image needs to be run through the same pipeline. \n",
    "5. Loss should be cross-entropy for the class predictions and mse for the coordinates. \n",
    "3. Change input size of the image to match Pedestrian dataset.\n",
    "5. Train the system on pedestrian dataset. \n",
    "5. Experiment with different feature map sizes (skinny is better I think.)\n",
    "5. Run predictions from test set to \n",
    "5. Experiment with different sizes to see if it varies the accuracy... \n",
    "6. Can I load the imagenet weights into the system ? \n",
    "\n",
    "#### Initial System \n",
    "1. Get base network running.\n",
    "2. Get base loss and. \n",
    "3. No augmentation. \n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "2. Download base dataset and run it through it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_H = 300\n",
    "IMAGE_W = 300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-c006d4cdb479>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-c006d4cdb479>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    if (len(n_ksize) != 2) or\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import numpy as np\n",
    "\n",
    "class IllegalArgumentError(ValueError):\n",
    "    pass\n",
    "\n",
    "def conv_layer_optional_pooling(x_tensor,n_outputs,n_ksize,n_strides,name,\n",
    "                                padding_type=\"VALID\",pool_ksize=None,pool_strides=None,pool_name=None):\n",
    "    # Convolution layer with Relu \n",
    "    \"\"\"\n",
    "    x_tensor : input tensor\n",
    "    n_outputs: number of outputs of the convolutional layer\n",
    "    n_ksize: kernel size 2-d tuple \n",
    "    n_strides: 2-d tuple for convlution\n",
    "    padding_type: Type of padding. default is \"SAME\"\n",
    "    pool_ksize: kernel size 2-d tuple for max pool\n",
    "    pool_strides: stride 2-d tuple for max pool\n",
    "    \n",
    "    returns A tensor that is hte output of convolution, relu & max-pooling (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    # check for dumb input errors\n",
    "    if (len(n_ksize) != 2) or \n",
    "        (len(n_strides) !=2) or \n",
    "        (pool_ksize!=None and len(pool_ksize)!=2) or (pool_strides!=None and len(pool_strides!=2)):\n",
    "        raise IllegalArgumentError\n",
    "        \n",
    "    num_channels = int(x_tensor.shape[-1])\n",
    "    \n",
    "    filter_weight = tf.Variable(tf.truncated_normal(list(n_ksize)+[num_channels,n_outputs],mean=0,stddev=0.1))\n",
    "    filter_bias = tf.Variable(tf.zeros(n_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor,filter_weight,[1]+list(n_strides)+[1],padding_type,name=name)\n",
    "    conv_layer = tf.nn.bias_add(conv_layer,filter_bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    print(\"After conv_layer \",name,conv_layer.shape)\n",
    "    \n",
    "    if pool_ksize!= None and pool_strides !=None:\n",
    "            pooled = tf.nn.max_pool(conv_layer,\n",
    "                                    ksize=[1] + list(pool_ksize) + [1],\n",
    "                                    strides = [1] + list(pool_strides) + [1],\n",
    "                                    padding='SAME',name=pool_name)\n",
    "            print(\"After pool \",pool_,x.shape)\n",
    "            return pooled\n",
    "        \n",
    "    return conv_layer\n",
    "\n",
    "def flatten(x_tensor):\n",
    "    batch_size = x_tensor.shape[0]\n",
    "    for a in range(1,len(x_tensor.shape)):\n",
    "        mult = mult * int(x_tensor.shape[a])\n",
    "    return tf.reshape(x_tensor,[-1,mult])\n",
    "\n",
    "def fully_conn(x_tensor,num_outputs):\n",
    "    num_inputs = int(x_tensor.shape[1])\n",
    "    weight= tf.Variable(tf.random_normal([num_inputs,num_outputs],mean=0,stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(shape=num_outputs))\n",
    "    \n",
    "    layer = tf.add(tf.matmul(x_tensor,weight),bias)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "    \n",
    "    \n",
    "    \n",
    "def convolve_and_collect(x):\n",
    "    # Apply 2 convolutions\n",
    "    \n",
    "def VGGModel(x):\n",
    "    \"\"\"\n",
    "    This will have two outputs - the coordinates for all default boxes and confidences\n",
    "    \"\"\"\n",
    "    \n",
    "    x = conv_layer_optional_pooling(x,64,(3,3),(1,1),\"block1_conv1\",padding_type=\"SAME\")\n",
    "    x = conv_layer_optional_pooling(x,64,(3,3),(1,1),\"block1_conv2\",padding_type=\"SAME\",\n",
    "                                    pool_ksize=(2,2),pool_strides=(2,2),pool_name='block1_pool')\n",
    "    print(\"x after block1_pool\",x.shape)\n",
    "    \n",
    "    x = conv_layer_optional_pooling(x,128,(3,3),(1,1),\"block2_conv1\",padding_type=\"SAME\")\n",
    "    x = conv_layer_optional_pooling(x,128,(3,3),(1,1),\"block2_conv2\",padding_type=\"SAME\",\n",
    "                                    pool_ksize=(2,2),pool_strides=(2,2),pool_name='block2_pool')\n",
    "    print(\"x after block2_pool\",x.shape)\n",
    "    \n",
    "    x = conv_layer_optional_pooling(x,256,(3,3),(1,1),\"block3_conv1\",padding_type=\"SAME\")\n",
    "    x = conv_layer_optional_pooling(x,256,(3,3),(1,1),\"block3_conv2\",padding_type=\"SAME\")\n",
    "    x = conv_layer_optional_pooling(x,256,(3,3),(1,1),\"block3_conv3\",padding_type=\"SAME\",\n",
    "                                    pool_ksize=(2,2),pool_strides=(2,2),pool_name='block3_pool')\n",
    "    print(\"x after block3_pool\",x.shape)\n",
    "    \n",
    "    x = conv_layer_optional_pooling(x,512,(3,3),(1,1),\"block4_conv1\",padding_type=\"SAME\")\n",
    "    x = conv_layer_optional_pooling(x,512,(3,3),(1,1),\"block4_conv2\",padding_type=\"SAME\")\n",
    "    x = conv_layer_optional_pooling(x,512,(3,3),(1,1),\"block4_conv3\",padding_type=\"SAME\",\n",
    "                                    pool_ksize=(2,2),pool_strides=(2,2),pool_name='block4_pool')\n",
    "    \n",
    "    print(\"x after block4_pool\",x.shape)\n",
    "    # Need the final 3 layers with the image maps\n",
    "\n",
    "    # Add slowly reducing layers after this till we get to 1 pixel size region. \n",
    "    x = flatten(x)\n",
    "    \n",
    "def getData():\n",
    "    \"\"\"\n",
    "    Get the data and create a hash of all whats needed\n",
    "    \n",
    "    \"\"\"\n",
    "    for \n",
    "    \n",
    "def train(x):\n",
    "    \n",
    "    # Get the input \n",
    "    \n",
    "    x = tf.placeholder(tf.float32,shape=(None,IMG_W,IMAGE_H,name='x'))\n",
    "    y = tf.placeholder(tf.float32,shape=(None,TOTAL_Y_LEN,name='y'))\n",
    "    \n",
    "    # Inititalize the model\n",
    "    logits = VGGModel(x)\n",
    "    \n",
    "    # Figre out the costs. We will have a sum of two costs. \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logist=logits,labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    \n",
    "    with tf.Session as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                n_batches = 5 \n",
    "                for batch_i in range(1,n_batches+1):\n",
    "                    # Get the input\n",
    "                    # run the optimizer\n",
    "                    session.run(optimizer,feed_dict={x:,y:})\n",
    "        \n",
    "                       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test base network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
